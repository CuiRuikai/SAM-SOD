{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from methods.lora import Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from segment_anything.modeling import Sam\n",
    "from safetensors import safe_open\n",
    "from methods.segment_anything import sam_model_registry\n",
    "import loralib \n",
    "\n",
    "custom_config = {'base'      : {'strategy': 'base_adam',\n",
    "                                'batch': 2,\n",
    "                               },\n",
    "                 'customized': {'--ckpt_path': {'type': str, 'default': './ckpts/sam_vit_b_01ec64.pth'},\n",
    "                                '--model_type': {'type': str, 'default': 'vit_b'},\n",
    "                                '--rank': {'type': int, 'default': 4},\n",
    "                               },\n",
    "                }\n",
    "\n",
    "\n",
    "class _AdaLoRA_qkv(nn.Module):\n",
    "    def __init__(self, qkv, linear_q, linear_v):\n",
    "        super().__init__()\n",
    "        self.qkv = qkv\n",
    "        self.linear_q = linear_q\n",
    "        self.linear_v = linear_v\n",
    "        self.dim = qkv.in_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.qkv(x)  # B,N,N,3*org_C\n",
    "        new_q = self.linear_q(x)\n",
    "        new_v = self.linear_v(x)\n",
    "        qkv[:, :, :, : self.dim] += new_q\n",
    "        qkv[:, :, :, -self.dim:] += new_v\n",
    "        return qkv\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, config, encoder, feat):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        sam_checkpoint = config['ckpt_path']\n",
    "        model_type = config['model_type']\n",
    "        sam_model = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "\n",
    "        r = config['rank']\n",
    "\n",
    "        assert r > 0\n",
    "        # base_vit_dim = sam_model.image_encoder.patch_embed.proj.out_channels\n",
    "        # dim = base_vit_dim\n",
    "        self.lora_layer = list(range(len(sam_model.image_encoder.blocks)))  # Only apply lora to the image encoder by default\n",
    "        # create for storage, then we can init them or load weights\n",
    "\n",
    "        # lets freeze first\n",
    "        for param in sam_model.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Here, we do the surgery\n",
    "        for t_layer_i, blk in enumerate(sam_model.image_encoder.blocks):\n",
    "            # If we only want few lora layer instead of all\n",
    "            if t_layer_i not in self.lora_layer:\n",
    "                continue\n",
    "            w_qkv_linear = blk.attn.qkv\n",
    "            self.dim = w_qkv_linear.in_features\n",
    "            \n",
    "            linear_q = loralib.SVDLinear(in_features=self.dim, out_features=self.dim, r=r, bias=False)\n",
    "            linear_v = loralib.SVDLinear(in_features=self.dim, out_features=self.dim, r=r, bias=False)\n",
    "            blk.attn.qkv = _AdaLoRA_qkv(\n",
    "                w_qkv_linear,\n",
    "                linear_q,\n",
    "                linear_v\n",
    "            )\n",
    "        self.sam = sam_model\n",
    "        loralib.mark_only_lora_as_trainable(self.sam)\n",
    "\n",
    "\n",
    "    def forward(self, x, phase='test'):\n",
    "        batched_input = x\n",
    "        image_size = batched_input.shape[-1]\n",
    "        out = self.sam(batched_input, multimask_output=False, image_size=image_size)\n",
    "        out_dict = {'sal': out['masks'], 'final': out['low_res_logits']}\n",
    "        return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loralib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 7\u001B[0m\n\u001B[1;32m      1\u001B[0m config \u001B[39m=\u001B[39m {\u001B[39m'\u001B[39m\u001B[39mckpt_path\u001B[39m\u001B[39m'\u001B[39m: \u001B[39m'\u001B[39m\u001B[39m./ckpts/sam_vit_b_01ec64.pth\u001B[39m\u001B[39m'\u001B[39m,\n\u001B[1;32m      2\u001B[0m           \u001B[39m'\u001B[39m\u001B[39mmodel_type\u001B[39m\u001B[39m'\u001B[39m: \u001B[39m'\u001B[39m\u001B[39mvit_b\u001B[39m\u001B[39m'\u001B[39m,\n\u001B[1;32m      3\u001B[0m           \u001B[39m'\u001B[39m\u001B[39mtrain_encoder\u001B[39m\u001B[39m'\u001B[39m: \u001B[39mFalse\u001B[39;00m,\n\u001B[1;32m      4\u001B[0m           \u001B[39m'\u001B[39m\u001B[39mtrain_decoder\u001B[39m\u001B[39m'\u001B[39m: \u001B[39mFalse\u001B[39;00m,\n\u001B[1;32m      5\u001B[0m           \u001B[39m'\u001B[39m\u001B[39mtrain_prompt\u001B[39m\u001B[39m'\u001B[39m: \u001B[39mFalse\u001B[39;00m,\n\u001B[1;32m      6\u001B[0m           \u001B[39m'\u001B[39m\u001B[39mrank\u001B[39m\u001B[39m'\u001B[39m: \u001B[39m4\u001B[39m}\n\u001B[0;32m----> 7\u001B[0m sam \u001B[39m=\u001B[39m Network(config\u001B[39m=\u001B[39;49mconfig, encoder\u001B[39m=\u001B[39;49m\u001B[39mNone\u001B[39;49;00m, feat\u001B[39m=\u001B[39;49m\u001B[39mNone\u001B[39;49;00m)\u001B[39m.\u001B[39mcuda()\n",
      "Cell \u001B[0;32mIn[1], line 65\u001B[0m, in \u001B[0;36mNetwork.__init__\u001B[0;34m(self, config, encoder, feat)\u001B[0m\n\u001B[1;32m     62\u001B[0m w_qkv_linear \u001B[39m=\u001B[39m blk\u001B[39m.\u001B[39mattn\u001B[39m.\u001B[39mqkv\n\u001B[1;32m     63\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdim \u001B[39m=\u001B[39m w_qkv_linear\u001B[39m.\u001B[39min_features\n\u001B[0;32m---> 65\u001B[0m linear_q \u001B[39m=\u001B[39m loralib\u001B[39m.\u001B[39mSVDLinear(in_features\u001B[39m=\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdim, out_features\u001B[39m=\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdim, r\u001B[39m=\u001B[39mr, bias\u001B[39m=\u001B[39m\u001B[39mFalse\u001B[39;00m)\n\u001B[1;32m     66\u001B[0m linear_v \u001B[39m=\u001B[39m loralib\u001B[39m.\u001B[39mSVDLinear(in_features\u001B[39m=\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdim, out_features\u001B[39m=\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdim, r\u001B[39m=\u001B[39mr, bias\u001B[39m=\u001B[39m\u001B[39mFalse\u001B[39;00m)\n\u001B[1;32m     67\u001B[0m blk\u001B[39m.\u001B[39mattn\u001B[39m.\u001B[39mqkv \u001B[39m=\u001B[39m _AdaLoRA_qkv(\n\u001B[1;32m     68\u001B[0m     w_qkv_linear,\n\u001B[1;32m     69\u001B[0m     linear_q,\n\u001B[1;32m     70\u001B[0m     linear_v\n\u001B[1;32m     71\u001B[0m )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'loralib' is not defined"
     ]
    }
   ],
   "source": [
    "config = {'ckpt_path': './ckpts/sam_vit_b_01ec64.pth',\n",
    "          'model_type': 'vit_b',\n",
    "          'train_encoder': False,\n",
    "          'train_decoder': False,\n",
    "          'train_prompt': False,\n",
    "          'rank': 4}\n",
    "sam = Network(config=config, encoder=None, feat=None).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sam.image_encoder.blocks.0.attn.qkv.linear_q.lora_A\n",
      "sam.image_encoder.blocks.0.attn.qkv.linear_q.lora_E\n",
      "sam.image_encoder.blocks.0.attn.qkv.linear_q.lora_B\n",
      "sam.image_encoder.blocks.0.attn.qkv.linear_v.lora_A\n",
      "sam.image_encoder.blocks.0.attn.qkv.linear_v.lora_E\n",
      "sam.image_encoder.blocks.0.attn.qkv.linear_v.lora_B\n",
      "sam.image_encoder.blocks.1.attn.qkv.linear_q.lora_A\n",
      "sam.image_encoder.blocks.1.attn.qkv.linear_q.lora_E\n",
      "sam.image_encoder.blocks.1.attn.qkv.linear_q.lora_B\n",
      "sam.image_encoder.blocks.1.attn.qkv.linear_v.lora_A\n",
      "sam.image_encoder.blocks.1.attn.qkv.linear_v.lora_E\n",
      "sam.image_encoder.blocks.1.attn.qkv.linear_v.lora_B\n",
      "sam.image_encoder.blocks.2.attn.qkv.linear_q.lora_A\n",
      "sam.image_encoder.blocks.2.attn.qkv.linear_q.lora_E\n",
      "sam.image_encoder.blocks.2.attn.qkv.linear_q.lora_B\n",
      "sam.image_encoder.blocks.2.attn.qkv.linear_v.lora_A\n",
      "sam.image_encoder.blocks.2.attn.qkv.linear_v.lora_E\n",
      "sam.image_encoder.blocks.2.attn.qkv.linear_v.lora_B\n",
      "sam.image_encoder.blocks.3.attn.qkv.linear_q.lora_A\n",
      "sam.image_encoder.blocks.3.attn.qkv.linear_q.lora_E\n",
      "sam.image_encoder.blocks.3.attn.qkv.linear_q.lora_B\n",
      "sam.image_encoder.blocks.3.attn.qkv.linear_v.lora_A\n",
      "sam.image_encoder.blocks.3.attn.qkv.linear_v.lora_E\n",
      "sam.image_encoder.blocks.3.attn.qkv.linear_v.lora_B\n",
      "sam.image_encoder.blocks.4.attn.qkv.linear_q.lora_A\n",
      "sam.image_encoder.blocks.4.attn.qkv.linear_q.lora_E\n",
      "sam.image_encoder.blocks.4.attn.qkv.linear_q.lora_B\n",
      "sam.image_encoder.blocks.4.attn.qkv.linear_v.lora_A\n",
      "sam.image_encoder.blocks.4.attn.qkv.linear_v.lora_E\n",
      "sam.image_encoder.blocks.4.attn.qkv.linear_v.lora_B\n",
      "sam.image_encoder.blocks.5.attn.qkv.linear_q.lora_A\n",
      "sam.image_encoder.blocks.5.attn.qkv.linear_q.lora_E\n",
      "sam.image_encoder.blocks.5.attn.qkv.linear_q.lora_B\n",
      "sam.image_encoder.blocks.5.attn.qkv.linear_v.lora_A\n",
      "sam.image_encoder.blocks.5.attn.qkv.linear_v.lora_E\n",
      "sam.image_encoder.blocks.5.attn.qkv.linear_v.lora_B\n",
      "sam.image_encoder.blocks.6.attn.qkv.linear_q.lora_A\n",
      "sam.image_encoder.blocks.6.attn.qkv.linear_q.lora_E\n",
      "sam.image_encoder.blocks.6.attn.qkv.linear_q.lora_B\n",
      "sam.image_encoder.blocks.6.attn.qkv.linear_v.lora_A\n",
      "sam.image_encoder.blocks.6.attn.qkv.linear_v.lora_E\n",
      "sam.image_encoder.blocks.6.attn.qkv.linear_v.lora_B\n",
      "sam.image_encoder.blocks.7.attn.qkv.linear_q.lora_A\n",
      "sam.image_encoder.blocks.7.attn.qkv.linear_q.lora_E\n",
      "sam.image_encoder.blocks.7.attn.qkv.linear_q.lora_B\n",
      "sam.image_encoder.blocks.7.attn.qkv.linear_v.lora_A\n",
      "sam.image_encoder.blocks.7.attn.qkv.linear_v.lora_E\n",
      "sam.image_encoder.blocks.7.attn.qkv.linear_v.lora_B\n",
      "sam.image_encoder.blocks.8.attn.qkv.linear_q.lora_A\n",
      "sam.image_encoder.blocks.8.attn.qkv.linear_q.lora_E\n",
      "sam.image_encoder.blocks.8.attn.qkv.linear_q.lora_B\n",
      "sam.image_encoder.blocks.8.attn.qkv.linear_v.lora_A\n",
      "sam.image_encoder.blocks.8.attn.qkv.linear_v.lora_E\n",
      "sam.image_encoder.blocks.8.attn.qkv.linear_v.lora_B\n",
      "sam.image_encoder.blocks.9.attn.qkv.linear_q.lora_A\n",
      "sam.image_encoder.blocks.9.attn.qkv.linear_q.lora_E\n",
      "sam.image_encoder.blocks.9.attn.qkv.linear_q.lora_B\n",
      "sam.image_encoder.blocks.9.attn.qkv.linear_v.lora_A\n",
      "sam.image_encoder.blocks.9.attn.qkv.linear_v.lora_E\n",
      "sam.image_encoder.blocks.9.attn.qkv.linear_v.lora_B\n",
      "sam.image_encoder.blocks.10.attn.qkv.linear_q.lora_A\n",
      "sam.image_encoder.blocks.10.attn.qkv.linear_q.lora_E\n",
      "sam.image_encoder.blocks.10.attn.qkv.linear_q.lora_B\n",
      "sam.image_encoder.blocks.10.attn.qkv.linear_v.lora_A\n",
      "sam.image_encoder.blocks.10.attn.qkv.linear_v.lora_E\n",
      "sam.image_encoder.blocks.10.attn.qkv.linear_v.lora_B\n",
      "sam.image_encoder.blocks.11.attn.qkv.linear_q.lora_A\n",
      "sam.image_encoder.blocks.11.attn.qkv.linear_q.lora_E\n",
      "sam.image_encoder.blocks.11.attn.qkv.linear_q.lora_B\n",
      "sam.image_encoder.blocks.11.attn.qkv.linear_v.lora_A\n",
      "sam.image_encoder.blocks.11.attn.qkv.linear_v.lora_E\n",
      "sam.image_encoder.blocks.11.attn.qkv.linear_v.lora_B\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for n, p in sam.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(n)\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 14, 14, 2304]) torch.Size([25, 14, 14, 768]) torch.Size([25, 14, 14, 768])\n",
      "torch.Size([25, 14, 14, 2304]) torch.Size([25, 14, 14, 768]) torch.Size([25, 14, 14, 768])\n",
      "torch.Size([1, 64, 64, 2304]) torch.Size([1, 64, 64, 768]) torch.Size([1, 64, 64, 768])\n",
      "torch.Size([25, 14, 14, 2304]) torch.Size([25, 14, 14, 768]) torch.Size([25, 14, 14, 768])\n",
      "torch.Size([25, 14, 14, 2304]) torch.Size([25, 14, 14, 768]) torch.Size([25, 14, 14, 768])\n",
      "torch.Size([1, 64, 64, 2304]) torch.Size([1, 64, 64, 768]) torch.Size([1, 64, 64, 768])\n",
      "torch.Size([25, 14, 14, 2304]) torch.Size([25, 14, 14, 768]) torch.Size([25, 14, 14, 768])\n",
      "torch.Size([25, 14, 14, 2304]) torch.Size([25, 14, 14, 768]) torch.Size([25, 14, 14, 768])\n",
      "torch.Size([1, 64, 64, 2304]) torch.Size([1, 64, 64, 768]) torch.Size([1, 64, 64, 768])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 768.00 MiB (GPU 0; 23.70 GiB total capacity; 20.36 GiB already allocated; 547.00 MiB free; 21.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[39], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m out \u001B[39m=\u001B[39m sam(torch\u001B[39m.\u001B[39;49mrandn(\u001B[39m1\u001B[39;49m, \u001B[39m3\u001B[39;49m, \u001B[39m224\u001B[39;49m, \u001B[39m224\u001B[39;49m)\u001B[39m.\u001B[39;49mcuda())\n",
      "File \u001B[0;32m~/miniconda3/envs/sam/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_pre_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[39mor\u001B[39;00m _global_backward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[39m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[39m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[36], line 79\u001B[0m, in \u001B[0;36mNetwork.forward\u001B[0;34m(self, x, phase)\u001B[0m\n\u001B[1;32m     77\u001B[0m batched_input \u001B[39m=\u001B[39m x\n\u001B[1;32m     78\u001B[0m image_size \u001B[39m=\u001B[39m batched_input\u001B[39m.\u001B[39mshape[\u001B[39m-\u001B[39m\u001B[39m1\u001B[39m]\n\u001B[0;32m---> 79\u001B[0m out \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49msam(batched_input, multimask_output\u001B[39m=\u001B[39;49m\u001B[39mFalse\u001B[39;49;00m, image_size\u001B[39m=\u001B[39;49mimage_size)\n\u001B[1;32m     80\u001B[0m out_dict \u001B[39m=\u001B[39m {\u001B[39m'\u001B[39m\u001B[39msal\u001B[39m\u001B[39m'\u001B[39m: out[\u001B[39m'\u001B[39m\u001B[39mmasks\u001B[39m\u001B[39m'\u001B[39m], \u001B[39m'\u001B[39m\u001B[39mfinal\u001B[39m\u001B[39m'\u001B[39m: out[\u001B[39m'\u001B[39m\u001B[39mlow_res_logits\u001B[39m\u001B[39m'\u001B[39m]}\n\u001B[1;32m     81\u001B[0m \u001B[39mreturn\u001B[39;00m out_dict\n",
      "File \u001B[0;32m~/miniconda3/envs/sam/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_pre_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[39mor\u001B[39;00m _global_backward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[39m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[39m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/project/SAM-SOD/methods/segment_anything/modeling/sam.py:57\u001B[0m, in \u001B[0;36mSam.forward\u001B[0;34m(self, batched_input, image_size, multimask_output)\u001B[0m\n\u001B[1;32m     55\u001B[0m     outputs \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mforward_test(batched_input, multimask_output)\n\u001B[1;32m     56\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[0;32m---> 57\u001B[0m     outputs \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mforward_train(batched_input, multimask_output, image_size)\n\u001B[1;32m     58\u001B[0m \u001B[39mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/project/SAM-SOD/methods/segment_anything/modeling/sam.py:62\u001B[0m, in \u001B[0;36mSam.forward_train\u001B[0;34m(self, batched_input, multimask_output, image_size)\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mforward_train\u001B[39m(\u001B[39mself\u001B[39m, batched_input, multimask_output, image_size):\n\u001B[1;32m     61\u001B[0m     input_images \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mpreprocess(batched_input)\n\u001B[0;32m---> 62\u001B[0m     image_embeddings \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mimage_encoder(input_images)\n\u001B[1;32m     63\u001B[0m     sparse_embeddings, dense_embeddings \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mprompt_encoder(\n\u001B[1;32m     64\u001B[0m         points\u001B[39m=\u001B[39m\u001B[39mNone\u001B[39;00m, boxes\u001B[39m=\u001B[39m\u001B[39mNone\u001B[39;00m, masks\u001B[39m=\u001B[39m\u001B[39mNone\u001B[39;00m\n\u001B[1;32m     65\u001B[0m     )\n\u001B[1;32m     66\u001B[0m     low_res_masks, iou_predictions \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mmask_decoder(\n\u001B[1;32m     67\u001B[0m         image_embeddings\u001B[39m=\u001B[39mimage_embeddings,\n\u001B[1;32m     68\u001B[0m         image_pe\u001B[39m=\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mprompt_encoder\u001B[39m.\u001B[39mget_dense_pe(),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     71\u001B[0m         multimask_output\u001B[39m=\u001B[39mmultimask_output\n\u001B[1;32m     72\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/sam/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_pre_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[39mor\u001B[39;00m _global_backward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[39m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[39m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/project/SAM-SOD/methods/segment_anything/modeling/image_encoder.py:112\u001B[0m, in \u001B[0;36mImageEncoderViT.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    109\u001B[0m     x \u001B[39m=\u001B[39m x \u001B[39m+\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mpos_embed\n\u001B[1;32m    111\u001B[0m \u001B[39mfor\u001B[39;00m blk \u001B[39min\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mblocks:\n\u001B[0;32m--> 112\u001B[0m     x \u001B[39m=\u001B[39m blk(x)\n\u001B[1;32m    114\u001B[0m x \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mneck(x\u001B[39m.\u001B[39mpermute(\u001B[39m0\u001B[39m, \u001B[39m3\u001B[39m, \u001B[39m1\u001B[39m, \u001B[39m2\u001B[39m))\n\u001B[1;32m    116\u001B[0m \u001B[39mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/miniconda3/envs/sam/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_pre_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[39mor\u001B[39;00m _global_backward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[39m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[39m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/project/SAM-SOD/methods/segment_anything/modeling/image_encoder.py:174\u001B[0m, in \u001B[0;36mBlock.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    171\u001B[0m     H, W \u001B[39m=\u001B[39m x\u001B[39m.\u001B[39mshape[\u001B[39m1\u001B[39m], x\u001B[39m.\u001B[39mshape[\u001B[39m2\u001B[39m]\n\u001B[1;32m    172\u001B[0m     x, pad_hw \u001B[39m=\u001B[39m window_partition(x, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mwindow_size)\n\u001B[0;32m--> 174\u001B[0m x \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mattn(x)\n\u001B[1;32m    175\u001B[0m \u001B[39m# Reverse window partition\u001B[39;00m\n\u001B[1;32m    176\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mwindow_size \u001B[39m>\u001B[39m \u001B[39m0\u001B[39m:\n",
      "File \u001B[0;32m~/miniconda3/envs/sam/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_pre_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[39mor\u001B[39;00m _global_backward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[39m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[39m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/project/SAM-SOD/methods/segment_anything/modeling/image_encoder.py:234\u001B[0m, in \u001B[0;36mAttention.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    231\u001B[0m attn \u001B[39m=\u001B[39m (q \u001B[39m*\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mscale) \u001B[39m@\u001B[39m k\u001B[39m.\u001B[39mtranspose(\u001B[39m-\u001B[39m\u001B[39m2\u001B[39m, \u001B[39m-\u001B[39m\u001B[39m1\u001B[39m)\n\u001B[1;32m    233\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39muse_rel_pos:\n\u001B[0;32m--> 234\u001B[0m     attn \u001B[39m=\u001B[39m add_decomposed_rel_pos(attn, q, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mrel_pos_h, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mrel_pos_w, (H, W), (H, W))\n\u001B[1;32m    236\u001B[0m attn \u001B[39m=\u001B[39m attn\u001B[39m.\u001B[39msoftmax(dim\u001B[39m=\u001B[39m\u001B[39m-\u001B[39m\u001B[39m1\u001B[39m)\n\u001B[1;32m    237\u001B[0m x \u001B[39m=\u001B[39m (attn \u001B[39m@\u001B[39m v)\u001B[39m.\u001B[39mview(B, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mnum_heads, H, W, \u001B[39m-\u001B[39m\u001B[39m1\u001B[39m)\u001B[39m.\u001B[39mpermute(\u001B[39m0\u001B[39m, \u001B[39m2\u001B[39m, \u001B[39m3\u001B[39m, \u001B[39m1\u001B[39m, \u001B[39m4\u001B[39m)\u001B[39m.\u001B[39mreshape(B, H, W, \u001B[39m-\u001B[39m\u001B[39m1\u001B[39m)\n",
      "File \u001B[0;32m~/project/SAM-SOD/methods/segment_anything/modeling/image_encoder.py:358\u001B[0m, in \u001B[0;36madd_decomposed_rel_pos\u001B[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001B[0m\n\u001B[1;32m    354\u001B[0m rel_h \u001B[39m=\u001B[39m torch\u001B[39m.\u001B[39meinsum(\u001B[39m\"\u001B[39m\u001B[39mbhwc,hkc->bhwk\u001B[39m\u001B[39m\"\u001B[39m, r_q, Rh)\n\u001B[1;32m    355\u001B[0m rel_w \u001B[39m=\u001B[39m torch\u001B[39m.\u001B[39meinsum(\u001B[39m\"\u001B[39m\u001B[39mbhwc,wkc->bhwk\u001B[39m\u001B[39m\"\u001B[39m, r_q, Rw)\n\u001B[1;32m    357\u001B[0m attn \u001B[39m=\u001B[39m (\n\u001B[0;32m--> 358\u001B[0m     attn\u001B[39m.\u001B[39;49mview(B, q_h, q_w, k_h, k_w) \u001B[39m+\u001B[39;49m rel_h[:, :, :, :, \u001B[39mNone\u001B[39;49;00m] \u001B[39m+\u001B[39;49m rel_w[:, :, :, \u001B[39mNone\u001B[39;49;00m, :]\n\u001B[1;32m    359\u001B[0m )\u001B[39m.\u001B[39mview(B, q_h \u001B[39m*\u001B[39m q_w, k_h \u001B[39m*\u001B[39m k_w)\n\u001B[1;32m    361\u001B[0m \u001B[39mreturn\u001B[39;00m attn\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 768.00 MiB (GPU 0; 23.70 GiB total capacity; 20.36 GiB already allocated; 547.00 MiB free; 21.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "out = sam(torch.randn(1, 3, 224, 224).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 224, 224])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['sal'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam.load_state_dict(state_dict=torch.load('./weight/sam/resnet50/base/sam_resnet50_base_10.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4740ebbeae212ac062e0886a30b7a4c765af42cebaa70134441eb154c2599149"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
